'''
训练数据集
'''

'''
目前已经讨论了验证数据集和测试数据集，接下来讨论训练数据集，
通常会围绕训练数据集进行一些操作，这些操作称为预处理，
注意：无论对训练数据进行什么样的预处理，这些处理也需要应用到验证数据、测试数据以及后续的预测数据上
'''

'''
神经网络通常在值范围为0到1或-1到1的数据上表现最佳，其中-1到1范围更为优选。
将数据中心化到0可以帮助模型训练，因为这可以减轻权重在某个方向上的偏置。

然而数据值不一定严格局限于-1到1之间，模型在稍微超出该范围或某些值远大于范围时也能表现良好，
这种情况通常发生在用权重乘以数据并将结果于偏置求和后，结果会传递给一个激活函数，许多激活函数在上述范围内表现良好，
例如softmax会输出一个0到1之间的概率向量，sigmoid的输出范围也是0到1，tanh的输出范围是-1到1
另一个原因是神经网络依赖大量的乘法运算，如果乘以大于1或小于-1的数字，结果的规模会比原始值更大，
而在-1到1的范围内，结果是一个分数，即更小的值，将训练数据中的大数值于权重相乘可能会导致浮点溢出或不稳定性，例如权重增长过快，
使用较小的数值更容易控制训练过程
'''

'''
数据预处理有很多方式，比如标准化、缩放、方差缩放、均值去除、非线性变换、针对异常值的缩放等，这里不做讨论，
仅举一个简单的例子：对于一个值范围在0到255之间的图像，将整个数据集除以255，使数据返回到0到1的范围，
也可以减去127.5，得到范围从-127.5到127.5，再除以127.5，使数据返回到-1到1的范围

同时也需要确保对所有的数据集（训练集、验证集、测试集）使用相同的方式（包括相同的缩放参数），
通常，应准备一个选择的缩放器，并在每个数据集上使用它的实例，
一旦训练完成并想用新的样本进行预测时，必须使用与训练、验证和测试数据相同的缩放器实例对这些新样本进行缩放，
并且当处理数据时，需要将缩放器对象与模型一起保存，并在预测时使用，否则结果可能会有所不同
'''

'''
在训练样本较少的情况下，可以使用数据增强。
'''
